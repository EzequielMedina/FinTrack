# ü§ñ An√°lisis y Diagn√≥stico del Chatbot-Service FinTrack

## üìä Estado Actual del Sistema

### ‚úÖ Servicios Activos
- **Chatbot Service**: Puerto 8090 (Saludable)
- **Ollama**: Puerto 11434 (Activo)
- **MySQL**: Puerto 3306 (Saludable)
- **Account Service**: Puerto 8082 (Saludable)
- **Transaction Service**: Puerto 8083 (Saludable)

---

## üîç Problemas Identificados

### 1. **Problema Principal: Configuraci√≥n de Ollama**

#### üö® Issue Cr√≠tico: Modelo No Descargado
```bash
# El contenedor Ollama est√° corriendo pero probablemente no tiene el modelo llama3:8b descargado
# Esto causa que las llamadas a /api/chat fallen o retornen vac√≠o
```

**Soluci√≥n:**
```bash
# Entrar al contenedor de Ollama y descargar el modelo
docker exec -it fintrack-ollama ollama pull llama3:8b

# O si prefieres un modelo m√°s ligero:
docker exec -it fintrack-ollama ollama pull llama3.2:3b
```

### 2. **Problemas de Configuraci√≥n en Docker Compose**

#### ‚ùå Configuraci√≥n Actual Faltante
Tu `docker-compose.yml` necesita estas mejoras:

```yaml
# Agregar al docker-compose.yml
ollama:
  image: ollama/ollama:latest
  container_name: fintrack-ollama
  ports:
    - "11434:11434"
  environment:
    OLLAMA_NUM_PARALLEL: 1          # Limitar concurrencia
    OLLAMA_MAX_LOADED_MODELS: 1     # Limitar modelos cargados
    OLLAMA_HOST: 0.0.0.0           # Permitir conexiones externas
  deploy:
    resources:
      limits:
        memory: 4G                  # Limitar RAM como solicitaste
      reservations:
        memory: 2G
  volumes:
    - ollama_models:/root/.ollama   # Persistir modelos descargados
  networks:
    - fintrack-network
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
    interval: 30s
    timeout: 10s
    retries: 3

chatbot-service:
  build:
    context: ./backend/services/chatbot-service
    dockerfile: Dockerfile
  container_name: fintrack-chatbot-service
  environment:
    DB_HOST: mysql
    DB_PORT: 3306
    DB_NAME: fintrack
    DB_USER: fintrack_user
    DB_PASSWORD: fintrack_password
    OLLAMA_HOST: http://ollama:11434    # ‚ö†Ô∏è CR√çTICO: Usar nombre del servicio
    OLLAMA_MODEL: llama3:8b            # O llama3.2:3b para menos RAM
    PORT: 8090
    GIN_MODE: release
  ports:
    - "8090:8090"
  depends_on:
    mysql:
      condition: service_healthy
    ollama:
      condition: service_started        # Esperar que Ollama est√© disponible
  networks:
    - fintrack-network

volumes:
  ollama_models:                        # Volumen para persistir modelos
    driver: local
```

### 3. **Problemas en el C√≥digo**

#### üîß Fix en `ollama_client.go`
```go
// Agregar mejor manejo de errores y logs
func (c *Client) Chat(ctx context.Context, systemPrompt string, userPrompt string) (string, error) {
    // ‚ö†Ô∏è AGREGAR: Log para debugging
    log.Printf("ü§ñ Ollama request to %s with model %s", c.host, c.model)
    
    payload := map[string]any{
        "model": c.model,
        "messages": []map[string]string{
            {"role": "system", "content": systemPrompt},
            {"role": "user", "content": userPrompt},
        },
        "stream": false,
        "options": map[string]any{
            "temperature": 0.7,
            "top_p": 0.9,
            "num_ctx": 2048,  // Reducir contexto para usar menos RAM
        },
    }
    
    b, _ := json.Marshal(payload)
    req, err := http.NewRequestWithContext(ctx, http.MethodPost, fmt.Sprintf("%s/api/chat", c.host), bytes.NewReader(b))
    if err != nil { 
        log.Printf("‚ùå Error creating request: %v", err)
        return "", err 
    }
    
    req.Header.Set("Content-Type", "application/json")
    resp, err := c.http.Do(req)
    if err != nil { 
        log.Printf("‚ùå Error calling Ollama: %v", err)
        return "", err 
    }
    defer resp.Body.Close()
    
    if resp.StatusCode != http.StatusOK { 
        log.Printf("‚ùå Ollama returned status %d", resp.StatusCode)
        return "", fmt.Errorf("ollama status %d", resp.StatusCode) 
    }
    
    var out struct {
        Message struct{ Content string `json:"content"` } `json:"message"`
        Response string `json:"response"`
        Error    string `json:"error,omitempty"`
    }
    
    if err := json.NewDecoder(resp.Body).Decode(&out); err != nil { 
        log.Printf("‚ùå Error decoding response: %v", err)
        return "", err 
    }
    
    if out.Error != "" {
        log.Printf("‚ùå Ollama error: %s", out.Error)
        return "", fmt.Errorf("ollama error: %s", out.Error)
    }
    
    result := out.Message.Content
    if result == "" {
        result = out.Response
    }
    
    log.Printf("‚úÖ Ollama response length: %d chars", len(result))
    return result, nil
}
```

#### üîß Fix en `chatbot_service_impl.go`
```go
// Mejorar el fallback cuando Ollama falla
func (s *ChatbotServiceImpl) HandleQuery(ctx context.Context, req ports.ChatQueryRequest) (ports.ChatQueryResponse, error) {
    // ... c√≥digo existente ...
    
    // ‚ö†Ô∏è MEJORAR: Llamada a Ollama con mejor fallback
    r, err := s.llm.Chat(ctx, system, user)
    if err != nil {
        // Log del error pero no fallar completamente
        log.Printf("‚ö†Ô∏è Ollama fall√≥, usando respuesta b√°sica: %v", err)
        r = "" // Forzar uso del fallback
    }
    
    // ‚ö†Ô∏è MEJORAR: Fallback m√°s inteligente
    if r == "" || strings.TrimSpace(r) == "" {
        reply = s.generateBasicReply(totals, byType, topMerchants, req.Message)
    } else { 
        reply = r 
    }
    
    // ... resto del c√≥digo ...
}

// Nuevo m√©todo para respuesta b√°sica inteligente
func (s *ChatbotServiceImpl) generateBasicReply(totals ports.FinancialTotals, byType map[string]float64, merchants []ports.MerchantTotal, message string) string {
    msg := strings.ToLower(message)
    
    // Detectar intenci√≥n de la pregunta
    if strings.Contains(msg, "gasto") || strings.Contains(msg, "gastado") {
        return fmt.Sprintf("üìä **Resumen de Gastos**\\n\\nTotal gastado: **$%.2f**\\nTotal ingresado: **$%.2f**\\n\\nüè™ **Principales comercios:**\\n%s\\n\\nüí° *Tip: Puedes generar un reporte PDF para ver m√°s detalles.*", 
            totals.Expenses, totals.Incomes, formatMerchantsMarkdown(merchants))
    }
    
    if strings.Contains(msg, "ingreso") || strings.Contains(msg, "ganancia") {
        return fmt.Sprintf("üí∞ **Resumen de Ingresos**\\n\\nTotal ingresado: **$%.2f**\\nTotal gastado: **$%.2f**\\nBalance: **$%.2f**", 
            totals.Incomes, totals.Expenses, totals.Incomes-totals.Expenses)
    }
    
    if strings.Contains(msg, "tarjeta") {
        ccCharges := getVal(byType, "credit_charge")
        return fmt.Sprintf("üí≥ **Resumen de Tarjetas**\\n\\nConsumos con tarjeta de cr√©dito: **$%.2f**\\n\\nüí° *Tip: Puedes ver un gr√°fico por tarjeta usando 'Mostrar gr√°fico'.*", ccCharges)
    }
    
    // Respuesta general
    return fmt.Sprintf("üìà **Resumen Financiero**\\n\\n‚Ä¢ Total gastado: **$%.2f**\\n‚Ä¢ Total ingresado: **$%.2f**\\n‚Ä¢ Balance: **$%.2f**\\n\\nüè™ **Top comercios:** %s\\n\\nüí° *¬øTe gustar√≠a generar un reporte PDF o ver gr√°ficos de tus movimientos?*",
        totals.Expenses, totals.Incomes, totals.Incomes-totals.Expenses, formatMerchantsSimple(merchants))
}
```

---

## üß™ Pruebas y Diagn√≥stico

### 1. **Verificar Estado de Ollama**

```bash
# Test 1: Verificar que Ollama responde
curl http://localhost:11434/api/tags

# Test 2: Verificar modelos instalados
docker exec fintrack-ollama ollama list

# Test 3: Descargar modelo si no existe
docker exec fintrack-ollama ollama pull llama3.2:3b

# Test 4: Probar chat directo
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2:3b",
  "messages": [{"role": "user", "content": "Hola, responde en espa√±ol"}],
  "stream": false
}'
```

### 2. **Verificar Chatbot Service**

```bash
# Test 1: Health check
curl http://localhost:8090/health

# Test 2: Consulta b√°sica
curl -X POST http://localhost:8090/api/chat/query \\
  -H "Content-Type: application/json" \\
  -d '{
    "userId": "test-user-123",
    "message": "¬øCu√°nto gast√© este mes?",
    "period": {"from": "2025-10-01", "to": "2025-10-31"}
  }'

# Test 3: Verificar logs del contenedor
docker logs fintrack-chatbot-service -f
```

### 3. **Verificar Datos en Base de Datos**

```sql
-- Conectar a MySQL y verificar que hay datos de prueba
SELECT COUNT(*) FROM transactions WHERE user_id = 'test-user-123';
SELECT COUNT(*) FROM accounts WHERE user_id = 'test-user-123';
```

---

## ‚ö° Soluci√≥n R√°pida (Quick Fix)

### 1. **Recrear con Configuraci√≥n Correcta**

```bash
# 1. Detener servicios
docker-compose down

# 2. Agregar configuraci√≥n de Ollama al docker-compose.yml (ver arriba)

# 3. Reiniciar todo
docker-compose up --build -d

# 4. Descargar modelo Ollama
docker exec fintrack-ollama ollama pull llama3.2:3b

# 5. Verificar que funciona
curl http://localhost:8090/health
```

### 2. **Configuraci√≥n Alternativa (Modelo M√°s Liviano)**

Si tienes problemas de RAM, usa un modelo m√°s peque√±o:

```yaml
# En docker-compose.yml
chatbot-service:
  environment:
    OLLAMA_MODEL: llama3.2:1b  # Modelo m√°s peque√±o (1B par√°metros)
    # O usar qwen2.5:0.5b para a√∫n menos RAM
```

---

## üîß Mejoras Recomendadas

### 1. **Sistema de Fallback Robusto**

```go
// Implementar en chatbot_service_impl.go
type ChatbotServiceImpl struct {
    data   ports.DataProvider
    llm    ports.OllamaProvider
    report ports.ReportProvider
    fallbackEnabled bool  // ‚ö†Ô∏è NUEVO: Flag para activar fallback
}

func (s *ChatbotServiceImpl) HandleQuery(ctx context.Context, req ports.ChatQueryRequest) (ports.ChatQueryResponse, error) {
    // Intentar Ollama primero
    reply, err := s.tryOllamaQuery(ctx, req)
    if err != nil && s.fallbackEnabled {
        // Usar respuesta estructurada como fallback
        reply = s.generateStructuredReply(ctx, req)
    }
    
    return ports.ChatQueryResponse{
        Reply: reply,
        SuggestedActions: s.generateSuggestedActions(req),
        Insights: s.generateInsights(ctx, req),
        DataRefs: s.gatherDataRefs(ctx, req),
    }, nil
}
```

### 2. **Configuraci√≥n de Environment**

```bash
# Agregar al .env del chatbot-service
CHATBOT_FALLBACK_ENABLED=true
CHATBOT_MAX_CONTEXT_LENGTH=2048
CHATBOT_RESPONSE_TIMEOUT=30s
OLLAMA_TEMPERATURE=0.7
OLLAMA_TOP_P=0.9
```

### 3. **Health Check Mejorado**

```go
// En chat_handler.go
func (h *ChatHandler) Health(c *gin.Context) {
    // Verificar Ollama
    ollamaStatus := "unknown"
    if err := h.svc.TestOllamaConnection(); err == nil {
        ollamaStatus = "healthy"
    } else {
        ollamaStatus = "unhealthy: " + err.Error()
    }
    
    c.JSON(http.StatusOK, gin.H{
        "status": "healthy", 
        "service": "chatbot-service",
        "ollama": ollamaStatus,
        "timestamp": time.Now(),
    })
}
```

---

## üéØ Pasos Inmediatos para Solucionar

### ‚úÖ Checklist de Soluci√≥n

1. **[ ] Verificar modelo Ollama descargado**
   ```bash
   docker exec fintrack-ollama ollama list
   ```

2. **[ ] Descargar modelo si falta**
   ```bash
   docker exec fintrack-ollama ollama pull llama3.2:3b
   ```

3. **[ ] Verificar configuraci√≥n de red**
   - Asegurar que chatbot-service use `http://ollama:11434` (no localhost)

4. **[ ] Agregar logs de debugging**
   - Implementar logs en ollama_client.go como se muestra arriba

5. **[ ] Probar endpoint directamente**
   ```bash
   curl -X POST http://localhost:8090/api/chat/query -H "Content-Type: application/json" -d '{"userId":"test","message":"¬øCu√°nto gast√©?","period":{"from":"2025-10-01","to":"2025-10-31"}}'
   ```

6. **[ ] Verificar logs del contenedor**
   ```bash
   docker logs fintrack-chatbot-service -f
   ```

### üî• Si Todo Falla - Modo Debugging

```bash
# 1. Entrar al contenedor del chatbot
docker exec -it fintrack-chatbot-service sh

# 2. Verificar conectividad a Ollama
curl http://ollama:11434/api/tags

# 3. Verificar variables de entorno
env | grep OLLAMA

# 4. Verificar conectividad a MySQL
nc -zv mysql 3306
```

---

## üí° Pr√≥ximos Pasos

Una vez que el chatbot funcione b√°sicamente, te recomiendo:

1. **Implementar cache** para respuestas frecuentes
2. **Agregar sistema de contexto** para conversaciones multi-turno
3. **Implementar rate limiting** para evitar sobrecarga
4. **Agregar m√©tricas** de performance y uso
5. **Crear interfaz de chat** en el frontend Angular

¬øTe gustar√≠a que implemente alguna de estas soluciones espec√≠ficas o prefieres que revisemos los logs para diagnosticar el problema actual?